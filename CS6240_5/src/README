Prerequisites to run on local cluster:

1. export HADOOP_CLASSPATH=.:`hadoop classpath`
2. If a tmp directory path is specified in core-site.xml, then please delete that tmp folder.
If no path is specified(as per instructions), then the script will handle deletions 
3. export HADOOP_HOME=/usr/local/hadoop
4. export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
5. Other basic hadoop setup on local machine (make sure datanode and namenode are running properly)
6. Input data (i.e. all folder after extracting tar.gz) must be present at current working directory (where make commands are being executed)

Prerequisites to run on EMR (Optional):

1. We are assuming that jobs are being run at your EMR 
2. In your aws configure make sure your output format is "json". We are using json parser in our script so if aws output is text then there will be a problem.
Type aws configure on command prompt
	AccessId : Enter 
	SecretKey : Enter
	region : us-west-2
	output format : json
3. Keep all folder in your current working directory frome where you will call make emr. 
4. We are pushing input data to your S3 bucket from our script. In case internet connection losses or problem happens please upload 
   input data to S3://adibpuri777/input. Output gets generated at s3://adibpuri777/output

=================================== Run ======================================

make run : It will create a HDFS file system, start hadoop, run your job, get the output in the solution_final

optional -> make clean emr : It will run your code on amazon cluster

================================== Output ===================================

The following is the output generated for 298.csv.gz in solution_final:

Carrier_Code    Year    Connection      MissedConnection        Percentage
OO		2015	809660		77082   		9.52029
MQ		2015	688052		57960   		8.42378
NK		2015	34550		3099    		8.96961
B6		2015	225131		16485   		7.3224
UA		2015	788657		49458   		6.27117
AS		2015	135613		8052    		5.93748
VX		2015	25446		1635    		6.42537
HA		2015	91333		4297    		4.70476
WN		2015	2326906		110406  		4.74476
AA		2015	1746964 	103657  		5.93355
F9		2015	38548   	4851    		12.5843
EV		2015	967322  	73814   		7.63076
DL		2015	3836174 	154285  		4.02185
US		2015	937179  	48540   		5.17937

=================================== Design ==================================

1. Map function in the mapper class parses the input record and does sanity check and for each flight that 
   passes those check along with being not cancelled, it emits:
	- Arrival details of the flight by <CarrierCode-Year-AirportId, ScheduledArrivalTime-ActualArrivalTime>
	- Departure details of the flught by <CarrierCode-Year-AirportId, ScheduledDepartureTime-ActualDepartureTime>
2. These times' are in epoch minutes calculated by various methods in the Mapper
3. Reduce function in the Reducer class iterates over these Keys and caches them in a List (since Iterable can be used once)
4. The lists are then iterated to find connections along with the missed ones and are emitted from Reducer
5. Filter script consolidates this data according to each airline and year along with percentage of missed connections

=================================== Conclusion ==============================

From the output shown above for a small set of data, we can conclude that:
1. DL has the best ratio of missed connection to the total connection
2. F9 has the worst ratio of missed connection to the total connection
3. DL has the most connections available
So from the above deduction and considering the available data, we can conclude that DL is the best airline for connections
